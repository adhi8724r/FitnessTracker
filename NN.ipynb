{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ActivityDate</th>\n",
       "      <th>TotalSteps</th>\n",
       "      <th>TotalDistance</th>\n",
       "      <th>TrackerDistance</th>\n",
       "      <th>LoggedActivitiesDistance</th>\n",
       "      <th>VeryActiveDistance</th>\n",
       "      <th>ModeratelyActiveDistance</th>\n",
       "      <th>LightActiveDistance</th>\n",
       "      <th>SedentaryActiveDistance</th>\n",
       "      <th>VeryActiveMinutes</th>\n",
       "      <th>FairlyActiveMinutes</th>\n",
       "      <th>LightlyActiveMinutes</th>\n",
       "      <th>SedentaryMinutes</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>13162</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>328</td>\n",
       "      <td>728</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>4/13/2016</td>\n",
       "      <td>10735</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.69</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>217</td>\n",
       "      <td>776</td>\n",
       "      <td>1797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>4/14/2016</td>\n",
       "      <td>10460</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>181</td>\n",
       "      <td>1218</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>4/15/2016</td>\n",
       "      <td>9762</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>209</td>\n",
       "      <td>726</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>4/16/2016</td>\n",
       "      <td>12669</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>221</td>\n",
       "      <td>773</td>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>8877689391</td>\n",
       "      <td>5/8/2016</td>\n",
       "      <td>10686</td>\n",
       "      <td>8.110000</td>\n",
       "      <td>8.110000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>1174</td>\n",
       "      <td>2847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>8877689391</td>\n",
       "      <td>5/9/2016</td>\n",
       "      <td>20226</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.10</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.05</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>217</td>\n",
       "      <td>1131</td>\n",
       "      <td>3710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>8877689391</td>\n",
       "      <td>5/10/2016</td>\n",
       "      <td>10733</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.46</td>\n",
       "      <td>6.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>224</td>\n",
       "      <td>1187</td>\n",
       "      <td>2832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>8877689391</td>\n",
       "      <td>5/11/2016</td>\n",
       "      <td>21420</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>88</td>\n",
       "      <td>12</td>\n",
       "      <td>213</td>\n",
       "      <td>1127</td>\n",
       "      <td>3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>8877689391</td>\n",
       "      <td>5/12/2016</td>\n",
       "      <td>8064</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>770</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>940 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id ActivityDate  TotalSteps  TotalDistance  TrackerDistance  \\\n",
       "0    1503960366    4/12/2016       13162       8.500000         8.500000   \n",
       "1    1503960366    4/13/2016       10735       6.970000         6.970000   \n",
       "2    1503960366    4/14/2016       10460       6.740000         6.740000   \n",
       "3    1503960366    4/15/2016        9762       6.280000         6.280000   \n",
       "4    1503960366    4/16/2016       12669       8.160000         8.160000   \n",
       "..          ...          ...         ...            ...              ...   \n",
       "935  8877689391     5/8/2016       10686       8.110000         8.110000   \n",
       "936  8877689391     5/9/2016       20226      18.250000        18.250000   \n",
       "937  8877689391    5/10/2016       10733       8.150000         8.150000   \n",
       "938  8877689391    5/11/2016       21420      19.559999        19.559999   \n",
       "939  8877689391    5/12/2016        8064       6.120000         6.120000   \n",
       "\n",
       "     LoggedActivitiesDistance  VeryActiveDistance  ModeratelyActiveDistance  \\\n",
       "0                         0.0                1.88                      0.55   \n",
       "1                         0.0                1.57                      0.69   \n",
       "2                         0.0                2.44                      0.40   \n",
       "3                         0.0                2.14                      1.26   \n",
       "4                         0.0                2.71                      0.41   \n",
       "..                        ...                 ...                       ...   \n",
       "935                       0.0                1.08                      0.20   \n",
       "936                       0.0               11.10                      0.80   \n",
       "937                       0.0                1.35                      0.46   \n",
       "938                       0.0               13.22                      0.41   \n",
       "939                       0.0                1.82                      0.04   \n",
       "\n",
       "     LightActiveDistance  SedentaryActiveDistance  VeryActiveMinutes  \\\n",
       "0                   6.06                     0.00                 25   \n",
       "1                   4.71                     0.00                 21   \n",
       "2                   3.91                     0.00                 30   \n",
       "3                   2.83                     0.00                 29   \n",
       "4                   5.04                     0.00                 36   \n",
       "..                   ...                      ...                ...   \n",
       "935                 6.80                     0.00                 17   \n",
       "936                 6.24                     0.05                 73   \n",
       "937                 6.28                     0.00                 18   \n",
       "938                 5.89                     0.00                 88   \n",
       "939                 4.25                     0.00                 23   \n",
       "\n",
       "     FairlyActiveMinutes  LightlyActiveMinutes  SedentaryMinutes  Calories  \n",
       "0                     13                   328               728      1985  \n",
       "1                     19                   217               776      1797  \n",
       "2                     11                   181              1218      1776  \n",
       "3                     34                   209               726      1745  \n",
       "4                     10                   221               773      1863  \n",
       "..                   ...                   ...               ...       ...  \n",
       "935                    4                   245              1174      2847  \n",
       "936                   19                   217              1131      3710  \n",
       "937                   11                   224              1187      2832  \n",
       "938                   12                   213              1127      3832  \n",
       "939                    1                   137               770      1849  \n",
       "\n",
       "[940 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"dailyActivity_merged.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "x = df[['TotalSteps','TotalDistance','LightActiveDistance','VeryActiveMinutes','LightlyActiveMinutes','SedentaryMinutes']]\n",
    "y = df['Calories']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 5866350.5000 - mse: 5866350.5000 - val_loss: 5748734.5000 - val_mse: 5748734.5000\n",
      "Epoch 2/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5695394.5000 - mse: 5695394.5000 - val_loss: 5478131.5000 - val_mse: 5478131.5000\n",
      "Epoch 3/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5244108.5000 - mse: 5244108.5000 - val_loss: 3234920.7500 - val_mse: 3234920.7500\n",
      "Epoch 4/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2252301.2500 - mse: 2252301.2500 - val_loss: 963222.1875 - val_mse: 963222.1875\n",
      "Epoch 5/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1391845.8750 - mse: 1391845.8750 - val_loss: 827680.0625 - val_mse: 827680.0625\n",
      "Epoch 6/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 859869.6250 - mse: 859869.6250 - val_loss: 714042.5625 - val_mse: 714042.5625\n",
      "Epoch 7/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 725391.3125 - mse: 725391.3125 - val_loss: 581104.6250 - val_mse: 581104.6250\n",
      "Epoch 8/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 612197.7500 - mse: 612197.7500 - val_loss: 505401.8750 - val_mse: 505401.8750\n",
      "Epoch 9/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 586907.9375 - mse: 586907.9375 - val_loss: 429548.0000 - val_mse: 429548.0000\n",
      "Epoch 10/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 480094.5312 - mse: 480094.5312 - val_loss: 385321.0938 - val_mse: 385321.0938\n",
      "Epoch 11/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 483674.4062 - mse: 483674.4062 - val_loss: 359475.4375 - val_mse: 359475.4375\n",
      "Epoch 12/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 483983.4688 - mse: 483983.4688 - val_loss: 332035.0625 - val_mse: 332035.0625\n",
      "Epoch 13/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 450936.6250 - mse: 450936.6250 - val_loss: 317501.5000 - val_mse: 317501.5000\n",
      "Epoch 14/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 410733.5625 - mse: 410733.5625 - val_loss: 285752.8750 - val_mse: 285752.8750\n",
      "Epoch 15/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 357026.4375 - mse: 357026.4375 - val_loss: 283576.5625 - val_mse: 283576.5625\n",
      "Epoch 16/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 365339.0625 - mse: 365339.0625 - val_loss: 271102.5312 - val_mse: 271102.5312\n",
      "Epoch 17/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 349439.0938 - mse: 349439.0938 - val_loss: 249814.2344 - val_mse: 249814.2344\n",
      "Epoch 18/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 366862.6250 - mse: 366862.6250 - val_loss: 249609.7812 - val_mse: 249609.7812\n",
      "Epoch 19/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 345981.6562 - mse: 345981.6562 - val_loss: 239728.2500 - val_mse: 239728.2500\n",
      "Epoch 20/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 328692.1250 - mse: 328692.1250 - val_loss: 254753.4531 - val_mse: 254753.4531\n",
      "Epoch 21/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 307689.7812 - mse: 307689.7812 - val_loss: 243660.7812 - val_mse: 243660.7812\n",
      "Epoch 22/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 336920.0000 - mse: 336920.0000 - val_loss: 219146.5156 - val_mse: 219146.5156\n",
      "Epoch 23/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 321131.7188 - mse: 321131.7188 - val_loss: 222906.4531 - val_mse: 222906.4531\n",
      "Epoch 24/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 326071.5938 - mse: 326071.5938 - val_loss: 214473.1719 - val_mse: 214473.1719\n",
      "Epoch 25/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 312519.0000 - mse: 312519.0000 - val_loss: 228087.3438 - val_mse: 228087.3438\n",
      "Epoch 26/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 300360.5625 - mse: 300360.5625 - val_loss: 216828.1250 - val_mse: 216828.1250\n",
      "Epoch 27/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 300155.5312 - mse: 300155.5312 - val_loss: 202320.7656 - val_mse: 202320.7656\n",
      "Epoch 28/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 300909.0625 - mse: 300909.0625 - val_loss: 211862.3438 - val_mse: 211862.3438\n",
      "Epoch 29/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267401.0000 - mse: 267401.0000 - val_loss: 199836.6406 - val_mse: 199836.6406\n",
      "Epoch 30/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 295986.0000 - mse: 295986.0000 - val_loss: 202912.9844 - val_mse: 202912.9844\n",
      "Epoch 31/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 291423.8438 - mse: 291423.8438 - val_loss: 200633.8438 - val_mse: 200633.8438\n",
      "Epoch 32/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 296622.7188 - mse: 296622.7188 - val_loss: 194541.7812 - val_mse: 194541.7812\n",
      "Epoch 33/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266629.1250 - mse: 266629.1250 - val_loss: 198202.7188 - val_mse: 198202.7188\n",
      "Epoch 34/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 258007.4531 - mse: 258007.4531 - val_loss: 193022.7656 - val_mse: 193022.7656\n",
      "Epoch 35/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 277651.7188 - mse: 277651.7188 - val_loss: 200727.2812 - val_mse: 200727.2812\n",
      "Epoch 36/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 295382.9062 - mse: 295382.9062 - val_loss: 202195.8438 - val_mse: 202195.8438\n",
      "Epoch 37/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 282201.3438 - mse: 282201.3438 - val_loss: 210296.8750 - val_mse: 210296.8750\n",
      "Epoch 38/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 278270.6250 - mse: 278270.6250 - val_loss: 197405.5469 - val_mse: 197405.5469\n",
      "Epoch 39/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 292929.3438 - mse: 292929.3438 - val_loss: 199855.5156 - val_mse: 199855.5156\n",
      "Epoch 40/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 277440.5938 - mse: 277440.5938 - val_loss: 195249.9531 - val_mse: 195249.9531\n",
      "Epoch 41/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 257371.3281 - mse: 257371.3281 - val_loss: 186057.7656 - val_mse: 186057.7656\n",
      "Epoch 42/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 292065.0938 - mse: 292065.0938 - val_loss: 193376.8125 - val_mse: 193376.8125\n",
      "Epoch 43/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 315982.0938 - mse: 315982.0938 - val_loss: 184259.8438 - val_mse: 184259.8438\n",
      "Epoch 44/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 297226.4688 - mse: 297226.4688 - val_loss: 182857.6875 - val_mse: 182857.6875\n",
      "Epoch 45/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 281124.5625 - mse: 281124.5625 - val_loss: 186593.4531 - val_mse: 186593.4531\n",
      "Epoch 46/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 281420.3125 - mse: 281420.3125 - val_loss: 180914.8438 - val_mse: 180914.8438\n",
      "Epoch 47/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259325.5156 - mse: 259325.5156 - val_loss: 180611.3125 - val_mse: 180611.3125\n",
      "Epoch 48/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 242547.1562 - mse: 242547.1562 - val_loss: 181842.1875 - val_mse: 181842.1875\n",
      "Epoch 49/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 227791.1406 - mse: 227791.1406 - val_loss: 180802.3906 - val_mse: 180802.3906\n",
      "Epoch 50/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 282569.4062 - mse: 282569.4062 - val_loss: 180458.1250 - val_mse: 180458.1250\n",
      "Epoch 51/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 285555.0625 - mse: 285555.0625 - val_loss: 179014.5781 - val_mse: 179014.5781\n",
      "Epoch 52/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 225446.5156 - mse: 225446.5156 - val_loss: 180386.0625 - val_mse: 180386.0625\n",
      "Epoch 53/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 244848.8594 - mse: 244848.8594 - val_loss: 182356.9375 - val_mse: 182356.9375\n",
      "Epoch 54/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 251684.8906 - mse: 251684.8906 - val_loss: 174123.2188 - val_mse: 174123.2188\n",
      "Epoch 55/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 298351.8438 - mse: 298351.8438 - val_loss: 187872.1250 - val_mse: 187872.1250\n",
      "Epoch 56/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 276110.0938 - mse: 276110.0938 - val_loss: 173729.2500 - val_mse: 173729.2500\n",
      "Epoch 57/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264741.0625 - mse: 264741.0625 - val_loss: 184948.4531 - val_mse: 184948.4531\n",
      "Epoch 58/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 235289.7188 - mse: 235289.7188 - val_loss: 172011.2031 - val_mse: 172011.2031\n",
      "Epoch 59/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 251313.8125 - mse: 251313.8125 - val_loss: 173668.7969 - val_mse: 173668.7969\n",
      "Epoch 60/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 253406.7812 - mse: 253406.7812 - val_loss: 182611.1875 - val_mse: 182611.1875\n",
      "Epoch 61/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 303495.7812 - mse: 303495.7812 - val_loss: 175029.7812 - val_mse: 175029.7812\n",
      "Epoch 62/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 247770.2969 - mse: 247770.2969 - val_loss: 170761.4531 - val_mse: 170761.4531\n",
      "Epoch 63/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 269294.5938 - mse: 269294.5938 - val_loss: 178552.2500 - val_mse: 178552.2500\n",
      "Epoch 64/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 241382.5469 - mse: 241382.5469 - val_loss: 176229.1094 - val_mse: 176229.1094\n",
      "Epoch 65/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 281136.0000 - mse: 281136.0000 - val_loss: 168466.1094 - val_mse: 168466.1094\n",
      "Epoch 66/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262304.9062 - mse: 262304.9062 - val_loss: 168231.1562 - val_mse: 168231.1562\n",
      "Epoch 67/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 237269.9688 - mse: 237269.9688 - val_loss: 167945.7812 - val_mse: 167945.7812\n",
      "Epoch 68/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 245303.0938 - mse: 245303.0938 - val_loss: 169397.7812 - val_mse: 169397.7812\n",
      "Epoch 69/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 244902.4531 - mse: 244902.4531 - val_loss: 178218.3438 - val_mse: 178218.3438\n",
      "Epoch 70/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 236758.6406 - mse: 236758.6406 - val_loss: 170641.5781 - val_mse: 170641.5781\n",
      "Epoch 71/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 236880.4688 - mse: 236880.4688 - val_loss: 166565.7188 - val_mse: 166565.7188\n",
      "Epoch 72/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 268798.7812 - mse: 268798.7812 - val_loss: 167553.1562 - val_mse: 167553.1562\n",
      "Epoch 73/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 258821.7500 - mse: 258821.7500 - val_loss: 169559.7656 - val_mse: 169559.7656\n",
      "Epoch 74/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 230096.9375 - mse: 230096.9375 - val_loss: 164275.0781 - val_mse: 164275.0781\n",
      "Epoch 75/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 274732.7188 - mse: 274732.7188 - val_loss: 167527.4844 - val_mse: 167527.4844\n",
      "Epoch 76/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 255148.2188 - mse: 255148.2188 - val_loss: 164130.5469 - val_mse: 164130.5469\n",
      "Epoch 77/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 250849.1250 - mse: 250849.1250 - val_loss: 163539.2031 - val_mse: 163539.2031\n",
      "Epoch 78/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 220811.1875 - mse: 220811.1875 - val_loss: 173091.7344 - val_mse: 173091.7344\n",
      "Epoch 79/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 229943.2188 - mse: 229943.2188 - val_loss: 163308.0469 - val_mse: 163308.0469\n",
      "Epoch 80/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 228091.7969 - mse: 228091.7969 - val_loss: 164239.0625 - val_mse: 164239.0625\n",
      "Epoch 81/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 270932.9688 - mse: 270932.9688 - val_loss: 172287.6250 - val_mse: 172287.6250\n",
      "Epoch 82/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 240789.1875 - mse: 240789.1875 - val_loss: 159695.1875 - val_mse: 159695.1875\n",
      "Epoch 83/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267010.0625 - mse: 267010.0625 - val_loss: 168025.9844 - val_mse: 168025.9844\n",
      "Epoch 84/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 252097.8438 - mse: 252097.8438 - val_loss: 173244.1250 - val_mse: 173244.1250\n",
      "Epoch 85/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 242236.1094 - mse: 242236.1094 - val_loss: 161282.5469 - val_mse: 161282.5469\n",
      "Epoch 86/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 229726.0312 - mse: 229726.0312 - val_loss: 157954.2031 - val_mse: 157954.2031\n",
      "Epoch 87/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 216661.4375 - mse: 216661.4375 - val_loss: 163535.3438 - val_mse: 163535.3438\n",
      "Epoch 88/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 237613.6406 - mse: 237613.6406 - val_loss: 164784.1562 - val_mse: 164784.1562\n",
      "Epoch 89/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 220520.1875 - mse: 220520.1875 - val_loss: 156716.4375 - val_mse: 156716.4375\n",
      "Epoch 90/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 242238.8438 - mse: 242238.8438 - val_loss: 160224.3594 - val_mse: 160224.3594\n",
      "Epoch 91/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 251103.9062 - mse: 251103.9062 - val_loss: 169257.0625 - val_mse: 169257.0625\n",
      "Epoch 92/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 246565.4062 - mse: 246565.4062 - val_loss: 164649.7656 - val_mse: 164649.7656\n",
      "Epoch 93/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 251350.2656 - mse: 251350.2656 - val_loss: 178273.9375 - val_mse: 178273.9375\n",
      "Epoch 94/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 242949.6406 - mse: 242949.6406 - val_loss: 167280.0781 - val_mse: 167280.0781\n",
      "Epoch 95/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 239984.8906 - mse: 239984.8906 - val_loss: 156668.9062 - val_mse: 156668.9062\n",
      "Epoch 96/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 245631.7812 - mse: 245631.7812 - val_loss: 155713.4844 - val_mse: 155713.4844\n",
      "Epoch 97/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 256082.4062 - mse: 256082.4062 - val_loss: 158207.1094 - val_mse: 158207.1094\n",
      "Epoch 98/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 239725.2188 - mse: 239725.2188 - val_loss: 157195.1562 - val_mse: 157195.1562\n",
      "Epoch 99/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 227728.0625 - mse: 227728.0625 - val_loss: 156494.9844 - val_mse: 156494.9844\n",
      "Epoch 100/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 215699.9688 - mse: 215699.9688 - val_loss: 173426.9531 - val_mse: 173426.9531\n",
      "Epoch 101/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 215266.0469 - mse: 215266.0469 - val_loss: 155944.8750 - val_mse: 155944.8750\n",
      "Epoch 102/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 224163.5312 - mse: 224163.5312 - val_loss: 157267.0625 - val_mse: 157267.0625\n",
      "Epoch 103/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 232755.5781 - mse: 232755.5781 - val_loss: 155600.8906 - val_mse: 155600.8906\n",
      "Epoch 104/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 242079.1250 - mse: 242079.1250 - val_loss: 154477.1875 - val_mse: 154477.1875\n",
      "Epoch 105/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 242408.7344 - mse: 242408.7344 - val_loss: 155496.7500 - val_mse: 155496.7500\n",
      "Epoch 106/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 246903.5156 - mse: 246903.5156 - val_loss: 157034.5781 - val_mse: 157034.5781\n",
      "Epoch 107/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 224827.1250 - mse: 224827.1250 - val_loss: 158063.7188 - val_mse: 158063.7188\n",
      "Epoch 108/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 220721.7812 - mse: 220721.7812 - val_loss: 157886.2500 - val_mse: 157886.2500\n",
      "Epoch 109/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 208919.1094 - mse: 208919.1094 - val_loss: 158494.6094 - val_mse: 158494.6094\n",
      "Epoch 110/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 218146.7188 - mse: 218146.7188 - val_loss: 153778.9844 - val_mse: 153778.9844\n",
      "Epoch 111/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 213251.1875 - mse: 213251.1875 - val_loss: 159158.3750 - val_mse: 159158.3750\n",
      "Epoch 112/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 210897.3438 - mse: 210897.3438 - val_loss: 159384.8750 - val_mse: 159384.8750\n",
      "Epoch 113/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 236512.1094 - mse: 236512.1094 - val_loss: 159647.7812 - val_mse: 159647.7812\n",
      "Epoch 114/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 240453.7969 - mse: 240453.7969 - val_loss: 157095.1562 - val_mse: 157095.1562\n",
      "Epoch 115/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 223188.0000 - mse: 223188.0000 - val_loss: 164859.8438 - val_mse: 164859.8438\n",
      "Epoch 116/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 242478.4844 - mse: 242478.4844 - val_loss: 175896.7656 - val_mse: 175896.7656\n",
      "Epoch 117/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 212705.2812 - mse: 212705.2812 - val_loss: 169902.2969 - val_mse: 169902.2969\n",
      "Epoch 118/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 199728.1875 - mse: 199728.1875 - val_loss: 156376.8125 - val_mse: 156376.8125\n",
      "Epoch 119/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 231717.1719 - mse: 231717.1719 - val_loss: 158339.0469 - val_mse: 158339.0469\n",
      "Epoch 120/500\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 223593.3125 - mse: 223593.3125 - val_loss: 155068.0469 - val_mse: 155068.0469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=500, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 164954.9844 - mse: 164954.9844\n",
      "Test MSE: 153778.984375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "loss, mse = model.evaluate(x_test, y_test)\n",
    "print(f\"Test MSE: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2722.0244],\n",
       "       [2267.5044],\n",
       "       [1684.2251],\n",
       "       [2815.1882],\n",
       "       [3023.5518]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605    2536\n",
       "63     2902\n",
       "136    2100\n",
       "611    2450\n",
       "439    3013\n",
       "       ... \n",
       "265    1982\n",
       "109    1856\n",
       "77     2489\n",
       "213    1141\n",
       "755    2995\n",
       "Name: Calories, Length: 188, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test MSE: 154513.45324234053\n",
      "XGBoost Test MAE: 303.42420277696976\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# Create DMatrix for XGBoost (optional but recommended for performance)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Define XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'eval_metric': 'rmse',            # Root Mean Squared Error\n",
    "    'max_depth': 6,                   # Maximum depth of a tree\n",
    "    'eta': 0.1,                       # Learning rate\n",
    "    'subsample': 0.8,                 # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.8,          # Subsample ratio of columns when constructing each tree\n",
    "    'seed': 42                        # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # Number of boosting rounds\n",
    "model_xgb = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = model_xgb.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Test MSE: {mse_xgb}\")\n",
    "print(f\"XGBoost Test MAE: {mae_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalActiveMinutes'] = df['VeryActiveMinutes'] + df['FairlyActiveMinutes'] + df['LightlyActiveMinutes']\n",
    "df['ActivityRatio'] = df['VeryActiveMinutes'] / (df['SedentaryMinutes'] + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Feature selection\n",
    "x = df[['TotalSteps', 'TotalDistance', 'LightActiveDistance', 'VeryActiveMinutes', 'LightlyActiveMinutes', 'SedentaryMinutes', 'TotalActiveMinutes', 'ActivityRatio']]\n",
    "y = df['Calories']\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data scaling\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test MSE: 153483.26767784293\n",
      "XGBoost Test MAE: 298.18697016289894\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Prepare the data (same as before)\n",
    "x = df[['TotalSteps', 'TotalDistance', 'LightActiveDistance', 'VeryActiveMinutes', 'LightlyActiveMinutes', 'SedentaryMinutes', 'TotalActiveMinutes', 'ActivityRatio']]\n",
    "y = df['Calories']\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix for XGBoost (optional but recommended for performance)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Define XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'eval_metric': 'rmse',            # Root Mean Squared Error\n",
    "    'max_depth': 6,                   # Maximum depth of a tree\n",
    "    'eta': 0.1,                       # Learning rate\n",
    "    'subsample': 0.8,                 # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.8,          # Subsample ratio of columns when constructing each tree\n",
    "    'seed': 42                        # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # Number of boosting rounds\n",
    "model_xgb = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = model_xgb.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Test MSE: {mse_xgb}\")\n",
    "print(f\"XGBoost Test MAE: {mae_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2565.1377, 2312.0627, 1842.9941, 2777.1194, 3035.4927],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_xgb[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605    2536\n",
       "63     2902\n",
       "136    2100\n",
       "611    2450\n",
       "439    3013\n",
       "Name: Calories, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XGBoostModel.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_xgb,\"XGBoostModel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Expecting data to be a DMatrix object, got: ', <class 'pandas.core.frame.DataFrame'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_xgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\core.py:2356\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict with data.  The full model will be used unless `iteration_range` is\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;124;03mspecified, meaning user have to either slice the model or use the\u001b[39;00m\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;124;03m``best_iteration`` attribute to get prediction from best model returned from\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2353\u001b[0m \n\u001b[0;32m   2354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DMatrix):\n\u001b[1;32m-> 2356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting data to be a DMatrix object, got: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(data))\n\u001b[0;32m   2357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[0;32m   2358\u001b[0m     fn \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfeature_names\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Expecting data to be a DMatrix object, got: ', <class 'pandas.core.frame.DataFrame'>)"
     ]
    }
   ],
   "source": [
    "model_xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
